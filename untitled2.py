# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18xugg7TmDC6dncfed4Zcwt_ubgCQ2FZI
"""

import os
import pandas as pd
import gc
import numpy as np
import pickle


from catboost import CatBoostClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import FunctionTransformer



def read_csv(csv_path:str, chuksizq:int)->pd.DataFrame:

    return data

def read_and_prepare_dataset(path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int=1,
                             save_to_path=None, verbose: bool=False):
    """
    Читает и обрабатывает данные, возвращая готовый pd.DataFrame с признаками для обучения модели.
    """
    preprocessed_frames = []
    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)
                            if filename.startswith('processed')])

    for step in range(0, num_parts_total, num_parts_to_preprocess_at_once):
        chunks = dataset_paths[step: step + num_parts_to_preprocess_at_once]
        if verbose:
            #print('Reading chunks:\n')
            for chunk in chunks:
                print(chunk)
        res = []
        for chunk_path in chunks:
            #print('chunk_path', chunk_path)
            chunk = pd.read_parquet(chunk_path)
            res.append(chunk)


        transactions_frame = pd.concat(res).reset_index(drop=True)
        print(transactions_frame.columns.values)



        preprocessed_frames.append(transactions_frame)
    return pd.concat(preprocessed_frames)



def reduce_mem_usage(data):
    start_memory = data.memory_usage().sum() / 1024**2
    print(f"Initial memory usage: {start_memory:.2f} MB")

    int_type_dict = {
        (np.iinfo(np.int8).min,  np.iinfo(np.int8).max):  np.int8,
        (np.iinfo(np.int16).min, np.iinfo(np.int16).max): np.int16,
        (np.iinfo(np.int32).min, np.iinfo(np.int32).max): np.int32,
        (np.iinfo(np.int64).min, np.iinfo(np.int64).max): np.int64,
    }

    float_type_dict = {
        (np.finfo(np.float16).min, np.finfo(np.float16).max): np.float16,
        (np.finfo(np.float32).min, np.finfo(np.float32).max): np.float32,
        (np.finfo(np.float64).min, np.finfo(np.float64).max): np.float64,
    }

    data.columns = data.columns.str.strip()

    for column in data.columns:
        col_type = data[column].dtype

        if np.issubdtype(col_type, np.integer):
            c_min = data[column].min()
            c_max = data[column].max()
            dtype = next((v for k, v in int_type_dict.items() if k[0] <= c_min and k[1] >= c_max), None)
            if dtype:
                data[column] = data[column].astype(dtype)
        elif np.issubdtype(col_type, np.floating):
            c_min = data[column].min()
            c_max = data[column].max()
            dtype = next((v for k, v in float_type_dict.items() if k[0] <= c_min and k[1] >= c_max), None)
            if dtype:
                data[column] = data[column].astype(dtype)
        elif col_type == object:
            data[column] = data[column].astype('category')



    end_memory = data.memory_usage().sum() / 1024**2
    print(f"Final memory usage: {end_memory:.2f} MB")
    print(f"Reduced by {(start_memory - end_memory) / start_memory * 100:.1f}%")
    return data



data = read_and_prepare_dataset(path_to_dataset='/content/drive/MyDrive/Skillbox/Skillbox/saved_data', num_parts_to_preprocess_at_once=1, num_parts_total=12)
X = data.drop('flag', axis=1)
y = data['flag']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
import catboost


params = {
    'iterations': 768,
    'depth': 10,
    'learning_rate': 0.06552457960440336,
    'random_strength': 0.2991397355158974,
    'l2_leaf_reg': 4.281272265260282,
    'border_count': 173,
    'bootstrap_type': 'No'
}


pipeline = Pipeline([
    ('mem_usage', FunctionTransformer(reduce_mem_usage)),
    ('classification', CatBoostClassifier(**params, task_type='GPU'))
])
pipeline.fit(X_train, y_train)
predict_proba = pipeline.predict_proba(X_test)
print(roc_auc_score(y_test, predict_proba[:,1]))
predict = pipeline.predict(X_test)
predict_df = pd.DataFrame({'prediction':predict})
predict_df.to_csv('/content/drive/MyDrive/predict.csv')
with open('/content/drive/MyDrive/pipeline.pkl', 'wb') as file:
    pickle.dump(pipeline, file)

!pip install catboost